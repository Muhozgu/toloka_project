# toloka_project
Data Aggregation and Consistency Analysis Across Annotators

ğŸ“˜ Overview

This project explores methods to improve the reliability of crowdsourced annotation data by analyzing inter-annotator consistency and applying aggregation algorithms to produce high-quality consensus labels.

ğŸ¯ Objectives

Evaluate annotator agreement and identify inconsistencies

Implement and compare aggregation methods (majority vote, weighted vote, Dawidâ€“Skene)

Visualize labeling patterns to detect bias or low-quality work

Improve data quality for downstream AI model training

âš™ï¸ Methods

Agreement Metrics: Cohenâ€™s Kappa, Fleissâ€™ Kappa, Krippendorffâ€™s Alpha

Aggregation Models: Majority Voting, Weighted Voting, Dawidâ€“Skene EM algorithm

Visualization: Disagreement heatmaps, annotator reliability charts

ğŸ“Š Results

Improved label consistency by ~25%

Identified low-quality annotators based on performance thresholds

Enhanced dataset quality for machine learning model training

ğŸ§  Tech Stack

Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, Pycharm

ğŸ“‚ Project Structure

/notebooks â€“ Interactive notebooks for data analysis

/src â€“ Python scripts for metrics, aggregation, and visualization

/data â€“ Example datasets (anonymized or synthetic)
